# -*- coding: utf-8 -*-
"""GNN_Retriever.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BRp_nOmBHCHX9SNu5JJCn8w7g4W9OCPj

# Create Graph First
"""

import pandas as pd
import torch
from torch_geometric.data import Data
from collections import defaultdict
import torch
import torch.nn as nn
from torch_geometric.nn import GCNConv, to_hetero
from transformers import AutoModel, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import random
from tqdm import tqdm

# --- Load CSV ---
df = pd.read_csv("../../data/mined_data/full_graph.csv")  # entity_1, relationship, entity_2, label (optional)

# --- Create ID mappings ---
entity2id = defaultdict(lambda: len(entity2id))
relation2id = defaultdict(lambda: len(relation2id))

edges = []
edge_types = []

for row in df.itertuples():
    h = entity2id[row.entity_1]
    t = entity2id[row.entity_2]
    r = relation2id[row.relationship]

    # Add edge (unidirectional or bidirectional)
    edges.append((h, t))
    edge_types.append(r)

    if hasattr(row, "relationship") and row.label == 0:  # undirected
        edges.append((t, h))
        edge_types.append(r)

# --- Build edge_index ---
edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # [2, num_edges]

# --- Optional: encode relation type as edge_attr ---
edge_attr = torch.tensor(edge_types, dtype=torch.long)  # [num_edges]

# --- Optional: node features (dummy features for now) ---
num_nodes = len(entity2id)
x = torch.eye(num_nodes)  # Identity as node features [num_nodes, num_nodes]

# --- Create PyG Data object ---
graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)

"""# GNN Train"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class GCN(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, vector_emb_dim):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)
        self.linear = nn.Linear(out_channels, vector_emb_dim)
        self.relu = nn.ReLU()
        self.to(device)

    def forward(self, graph):
        x, edge_index = graph.x.to(device), graph.edge_index.to(device)
        x = self.conv1(x, edge_index)
        x = self.relu(x)
        x = self.relu(self.linear(self.conv2(x, edge_index)))
        return x  # Node embeddings: [num_nodes, D]

class TripleEmbedder(nn.Module):
    def __init__(self, node_embed_dim, num_rels):
        super().__init__()
        self.rel_embed = nn.Embedding(num_rels, node_embed_dim)

    def forward(self, head_ids, rel_ids, tail_ids, node_embeddings):
        h = node_embeddings[head_ids]
        r = self.rel_embed(torch.tensor(rel_ids)).to(device)
        t = node_embeddings[tail_ids]
        triple_embed = h + r + t  # Simple additive scoring
        return triple_embed  # [batch, D]

class QueryEncoder(nn.Module):
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name).to(device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def forward(self, query_texts):
        encoded = self.tokenizer(query_texts, return_tensors="pt", padding=True, truncation=True)
        out = self.bert(**encoded.to(device))
        return out.last_hidden_state[:, 0, :]  # [CLS] token

class MyGraphDataset(Dataset):
    def __init__(self, df):
        self.questions = df.Question.tolist()
        self.gold_triples = df.Gold_Triples.tolist()

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        question_emb = query_encoder(self.questions[idx])
        positive_triple = random.choice(self.gold_triples[idx]).tolist()
        while True:
          negative_triple = random.choice(train_df.Gold_Triples.to_list())[0].tolist()
          if negative_triple != positive_triple:
            return question_emb, positive_triple, negative_triple

def custom_collate_fn(batch):
    queries, positive_triple, negative_triple = zip(*batch)
    queries = torch.stack(queries)
    return queries, positive_triple, negative_triple

def create_triple_embeddings(triple_list, node_embeddings):
  embs = []
  for triple in triple_list:
    embs.append(triple_encoder(entity2id[triple[0]], relation2id[triple[1]], entity2id[triple[2]], node_embeddings))
  return torch.stack(embs).reshape(len(triple_list), 1, query_encoder.bert.config.hidden_size)

query_encoder = QueryEncoder()
gcn = GCN(in_channels=graph.x.size(1), hidden_channels=64, out_channels=128, vector_emb_dim=query_encoder.bert.config.hidden_size)
triple_encoder = TripleEmbedder(node_embed_dim=query_encoder.bert.config.hidden_size, num_rels=graph.num_edges)

train_df = pd.read_parquet("../../data/mined_data/train_gold.parquet")
test_df = pd.read_parquet("../../data/mined_data/test_gold.parquet")

train_dataset = MyGraphDataset(train_df)
test_dataset = MyGraphDataset(test_df)

train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate_fn)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)

optimizer = torch.optim.AdamW(
    list(gcn.parameters()) +
    list(query_encoder.parameters()) +
    list(triple_encoder.parameters()))

triplet_loss = nn.TripletMarginLoss()

for epoch in tqdm(range(10)):
    gcn.train()
    query_encoder.train()
    triple_encoder.train()

    loss_val = 0
    for batch in tqdm(train_dataloader):
      optimizer.zero_grad()

      node_embeddings = gcn(graph)
      query_embeddings = batch[0]  # [B, D]

      positive_triple_embeddings = create_triple_embeddings(batch[1], node_embeddings)
      negative_triple_embeddings = create_triple_embeddings(batch[2], node_embeddings)

      loss = triplet_loss(query_embeddings, positive_triple_embeddings, negative_triple_embeddings)
      loss_val += loss.item()

      loss.backward()
      optimizer.step()

    epoch_loss = loss_val / len(train_dataloader)
    print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}")
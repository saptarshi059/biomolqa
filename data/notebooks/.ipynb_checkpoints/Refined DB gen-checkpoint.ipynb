{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47838ed0-fbf9-479d-be25-8396416e5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "from Bio import Entrez, Medline\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import signal\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Set email address (required for NCBI API usage)\n",
    "Entrez.email = 'sks6765@psu.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea01064-aeba-438b-b03d-260fe35fc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_viz(dataframe, column1, column2, label=1):\n",
    "    # Creating an interactive graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    for index, row in dataframe.iterrows():\n",
    "        G.add_edge(row[column1], row[column2], label=row[label] if label != 1 else 1)\n",
    "    \n",
    "    # Add edge labels\n",
    "    edge_labels = {(u, v): d['label'] for u, v, d in G.edges(data=True)}\n",
    "    nx.set_edge_attributes(G, edge_labels, 'label')\n",
    "    \n",
    "    # Position nodes using spring layout\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Extract node and edge information\n",
    "    node_x = [pos[node][0] for node in G.nodes()]\n",
    "    node_y = [pos[node][1] for node in G.nodes()]\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_text = []\n",
    "    for edge in G.edges():\n",
    "        source, target = edge\n",
    "        x0, y0 = pos[source]\n",
    "        x1, y1 = pos[target]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "        edge_text.append(G.get_edge_data(*edge)['label'])  # Use existing edge labels\n",
    "    \n",
    "    # Create a Plotly figure\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        mode='lines',\n",
    "        line_shape='spline',\n",
    "        opacity=0.5,\n",
    "        hoverinfo='none'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        hovertext=[f'Node {node}' for node in G.nodes()]\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=[(pos[edge[0]][0] + pos[edge[1]][0]) / 2 for edge in G.edges()],\n",
    "        y=[(pos[edge[0]][1] + pos[edge[1]][1]) / 2 for edge in G.edges()],\n",
    "        mode='text',\n",
    "        text=edge_text,\n",
    "        textposition='middle center',\n",
    "        hoverinfo='none'\n",
    "    )])\n",
    "    \n",
    "    # Customize the layout\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        hovermode='x',\n",
    "        margin=dict(b=20, l=5, r=5, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb589ef-f268-4be7-81fd-dabe969a12d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DDI Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41a15e-93ba-4add-90c6-d568f3fd4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi = pd.read_csv(\"../base_data/bio-decagon-combo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb7bbc-35d7-4602-95ec-987c7ce468f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "def CID_information(CID: int, timeout: int = 2):\n",
    "\n",
    "    # Set the signal handler\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "    # Set the alarm\n",
    "    signal.alarm(timeout)\n",
    "\n",
    "    try:\n",
    "        cleaned_cid = int(re.sub(r\"CID[m|s]*0*\", \"\", CID))\n",
    "        compound = pcp.Compound.from_cid(cleaned_cid)\n",
    "        try:\n",
    "            compound_name = compound.synonyms[0]\n",
    "        except:\n",
    "            compound_name = compound.iupac_name\n",
    "        return (compound_name, compound.canonical_smiles)\n",
    "    except TimeoutError:\n",
    "        print(\"Timeout: Function took too long to complete.\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Disable the alarm\n",
    "        signal.alarm(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167663fb-85d3-411f-90ad-84793bb55d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=85\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Creating the graph from the ddi dataframe\n",
    "G = nx.from_pandas_edgelist(ddi, 'STITCH 1', 'STITCH 2')\n",
    "\n",
    "# Get the largest connected component\n",
    "largest_component = max(nx.connected_components(G), key=len)\n",
    "\n",
    "# Sample nodes from the largest component\n",
    "sample_size = 200\n",
    "sample_nodes = np.random.choice(list(largest_component), size=sample_size, replace=False)\n",
    "\n",
    "# Get the subgraph induced by the sampled nodes\n",
    "subgraph = G.subgraph(sample_nodes)\n",
    "\n",
    "# Convert the subgraph back to a DataFrame\n",
    "sampled_df = nx.to_pandas_edgelist(subgraph)\n",
    "\n",
    "sampled_df.rename(columns={\"source\": \"STITCH 1\", \"target\": \"STITCH 2\"}, inplace=True)\n",
    "\n",
    "# Merging the sampled df with base ddi to get the side effect names\n",
    "sampled_df = pd.merge(sampled_df, ddi).drop_duplicates(subset=[\"STITCH 1\", \"STITCH 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67c985-bbe0-44b1-85cd-ea1d5282e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check the output\n",
    "\n",
    "# Visualizing sampled_df as a graph to see that there are no disconnected edges\n",
    "create_graph_viz(sampled_df, \"STITCH 1\", \"STITCH 2\")\n",
    "\n",
    "all_drugs_according_to_df = len(set(sampled_df['STITCH 1'].to_list()).union(set(sampled_df['STITCH 2'].to_list())))\n",
    "\n",
    "# Verify that you get the same value from the sampled_df as a graph\n",
    "G = nx.from_pandas_edgelist(sampled_df, 'STITCH 1', 'STITCH 2')\n",
    "number_of_drug_nodes = len(G.nodes())\n",
    "\n",
    "assert all_drugs_according_to_df == number_of_drug_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109eb20-1e03-4d05-b0d6-56951c391fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(all_information):\n",
    "    pd.DataFrame(all_information, columns=[\"drug_1_CID\", \"drug_1_name\", \"drug_1_SMILES\", \"relationship\", \"drug_2_CID\", \"drug_2_name\", \n",
    "                                               \"drug_2_SMILES\"]).to_csv(\"DDI_subset.csv\", index=False)\n",
    "\n",
    "number_of_ddis_to_retain = 500\n",
    "all_information = []\n",
    "\n",
    "for idx, sample in tqdm(enumerate(sampled_df.itertuples(index=False))):    \n",
    "    drug_1_CID = sample[0]\n",
    "    drug_2_CID = sample[1]\n",
    "    relationship = sample[3]\n",
    "\n",
    "    drug_1_info = CID_information(drug_1_CID)\n",
    "    drug_2_info = CID_information(drug_2_CID)\n",
    "    \n",
    "    try:\n",
    "        # I want the drug name to not include any numbers or brackets, i.e., only keep regular words.\n",
    "        if (not re.search(r\"[\\W\\d]|(sulfate)\", drug_1_info[0])) and (not re.search(r\"[\\W\\d]|(sulfate)\", drug_2_info[0])):\n",
    "\n",
    "            # This ensures that when I am retrieving the background information, I will get 20 pubmed hits.\n",
    "            drug1_query = f\"{drug_1_info[0]} AND hasabstract AND Humans AND (AD OR AE OR PK OR PD OR CO OR TU OR DE)\"\n",
    "            with Entrez.esearch(db='pubmed', term=drug1_query, retmax=20, sort=\"relevance\") as handle:\n",
    "                paper_list_drug1 = Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "            drug2_query = f\"{drug_2_info[0]} AND hasabstract AND Humans AND (AD OR AE OR PK OR PD OR CO OR TU OR DE)\"\n",
    "            with Entrez.esearch(db='pubmed', term=drug2_query, retmax=20, sort=\"relevance\") as handle:\n",
    "                paper_list_drug2 = Entrez.read(handle)[\"IdList\"]\n",
    "            \n",
    "            if len(paper_list_drug1) == 20 and len(paper_list_drug2) == 20:            \n",
    "                all_information.append((drug_1_CID, drug_1_info[0], drug_1_info[1], relationship, drug_2_CID, drug_2_info[0], drug_2_info[1]))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # timeout after 50 API calls.\n",
    "    if (idx % 50 == 0) and (idx != 0):\n",
    "        print(f\"Samples collected so far: {len(all_information)}. Saving checkpoint and cooling down ...\")\n",
    "        save_dataframe(all_information)\n",
    "        time.sleep(5)        \n",
    "    \n",
    "    if len(all_information) == number_of_ddis_to_retain:\n",
    "        print(\"Necessary number of DDIs obtained... exiting\")\n",
    "        save_dataframe(all_information)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecf766-2dc6-4c8a-9c41-96070eb78024",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DPI Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cf39b-285c-43c9-ad23-8d5ac82491c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All drugs in my database: 105\n",
    "Drugs that have protein interactions: 67\n",
    "Drugs that have protein interactions according to the actions table: 52\n",
    "Total proteins found (from the interactions, i.e., dpis_needed): 563\n",
    "Total proteins I could query from STRING: 560\n",
    "\n",
    "Total DPIs obtained from all drugs (dpis_needed): 3578\n",
    "Total DPIs retained after querying STRING (annotated_dpis): 3539\n",
    "\n",
    "After merging with the actions table - which I need to do to get the Drug-Protein relationship, 72 proteins remain.\n",
    "\n",
    ">>Thus, finally, we have 105 total drugs, 52 that participate in DPI & 72 unique proteins.<<\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8250a4-1864-458f-97de-6ec4d642f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the DPIs and their annotations\n",
    "\n",
    "dpi = pd.read_csv(\"../base_data/bio-decagon-targets-all.csv\")\n",
    "ddi_subset = pd.read_csv(\"DDI_subset.csv\")\n",
    "\n",
    "all_drugs = np.unique(np.concatenate((ddi_subset[\"drug_1_CID\"].unique(), ddi_subset[\"drug_2_CID\"].unique())))\n",
    "\n",
    "# This will return less number of drugs since NOT ALL DRUGS HAVE DPIS!\n",
    "dpis_needed = dpi.query(\"STITCH in @all_drugs\")\n",
    "\n",
    "dpis = []\n",
    "for row in tqdm(dpis_needed.itertuples()):  \n",
    "    # Retrieving protein data from the given gene ID\n",
    "    url = f\"https://string-db.org/api/json/get_string_ids?identifiers={row.Gene}&species=9606\"\n",
    "    try:\n",
    "        response = requests.get(url).json()[0]\n",
    "        dpis.append((row.STITCH, response[\"stringId\"], response[\"preferredName\"], response[\"annotation\"]))  \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Saving so that I don't have to requery STRING\n",
    "pd.DataFrame(dpis, columns=[\"item_id_a\", \"item_id_b\", \"protein_name\", \"protein_desc\"]).to_csv(\"annotated_dpis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd54df5-1c23-430d-883d-ec5b5d4b2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dpis = pd.read_csv(\"annotated_dpis.csv\")\n",
    "actions_subset = pd.read_csv(\"../base_data/actions.csv\")\n",
    "merged = pd.merge(annotated_dpis, actions_subset)\n",
    "merged.rename(columns={\"item_id_a\": \"cid\", \"item_id_b\": \"stringId\"}, inplace=True)\n",
    "\n",
    "# Adding drug names to the DPI dataframe for completeness\n",
    "d1_names = ddi_subset[[\"drug_1_CID\", \"drug_1_name\"]].set_index(\"drug_1_CID\")[\"drug_1_name\"].to_dict()\n",
    "d2_names = ddi_subset[[\"drug_2_CID\", \"drug_2_name\"]].set_index(\"drug_2_CID\")[\"drug_2_name\"].to_dict()\n",
    "drug_cid_df = pd.DataFrame({**d1_names, **d2_names}.items(), columns=[\"cid\", \"drug_name\"])\n",
    "\n",
    "pd.merge(merged, drug_cid_df).drop_duplicates().to_csv(\"DPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9464f-4901-4e65-a0c1-8609f223b394",
   "metadata": {},
   "source": [
    "## Collecting Background information\n",
    "\n",
    "Keywords obtained from https://pubmed.ncbi.nlm.nih.gov/help/#proximity-searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240bb59-7f81-433f-9b6e-f5dc7b08936c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Drug info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234ddb6-5d00-4822-87c9-fb763913c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_subset = pd.read_csv(\"DDI_subset.csv\")\n",
    "all_drugs = np.unique(np.concatenate((ddi_subset[\"drug_1_name\"].unique(), ddi_subset[\"drug_2_name\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24a4f0-f9f6-4615-aace-b29120ea7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drug_info(drug: str):\n",
    "    '''\n",
    "    AD - Administration and Dosage\n",
    "    AE - Adverse Effects\n",
    "    PK - Pharmacokinetics\n",
    "    PD - Pharmacology\n",
    "    CO - Complications\n",
    "    TU - Therapeutic Use\n",
    "    DE - Drug Effects\n",
    "    '''\n",
    "    drug_query = f\"{drug} AND hasabstract AND Humans AND (AD OR AE OR PK OR PD OR CO OR TU OR DE)\"\n",
    "\n",
    "    # Getting relevant papers for given drug\n",
    "    with Entrez.esearch(db='pubmed', term=drug_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = \"\"\n",
    "    for paper_id, record in zip(paper_list, record_list):\n",
    "        try:\n",
    "            metadata.append((drug, paper_id, record[\"TI\"], record[\"AU\"], record[\"MH\"], f\"https://pubmed.ncbi.nlm.nih.gov/{paper_id}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with Path(f\"../background_information_data/drug_data/{drug}.txt\").open(\"w\") as output_file:\n",
    "        output_file.write(all_abstracts_string)\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022dca2d-fda4-4b50-b948-bcfca084b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all the relevant information \n",
    "metadata = []\n",
    "for drug in tqdm(all_drugs):\n",
    "    metadata.extend(create_drug_info(drug))\n",
    "\n",
    "pd.DataFrame(metadata, columns=[\"drug_name\", \"pubmed_id\", \"title\", \"authors\", \"mesh_terms\", \"paper_url\"]).to_csv(\"../background_information_data/drug_data/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93a1ac-d416-42de-829f-f89479b9e070",
   "metadata": {},
   "source": [
    "### Protein info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076b5c9-1f2b-4214-8f77-98b5def058aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi_subset = pd.read_csv(\"DPI_subset.csv\")\n",
    "all_proteins = dpi_subset[[\"protein_name\", \"protein_desc\"]].set_index(\"protein_name\")[\"protein_desc\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9d603-80ca-4372-90a4-b40346fe590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_protein_info(protein: str, protein_desc: str):\n",
    "    '''\n",
    "    CH - chemistry\n",
    "    ME - metabolism\n",
    "    PH - physiology\n",
    "    GE - genetics\n",
    "    '''\n",
    "    protein_query = f\"{protein} AND hasabstract AND Humans AND (CH OR ME OR PH OR GE)\"\n",
    "\n",
    "    # Getting relevant papers for given protein\n",
    "    with Entrez.esearch(db='pubmed', term=protein_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = protein_desc + \"\\n\" # Adding the annotation information from STRING\n",
    "    for paper_id, record in zip(paper_list, record_list):\n",
    "        try:\n",
    "            metadata.append((protein, paper_id, record[\"TI\"], record[\"AU\"], record[\"MH\"], f\"https://pubmed.ncbi.nlm.nih.gov/{paper_id}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with Path(f\"../background_information_data/protein_data/{protein}.txt\").open(\"w\") as output_file:\n",
    "        output_file.write(all_abstracts_string)\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e67b5-b70d-4826-aabe-620dc527b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for protein_name, protein_desc in tqdm(all_proteins.items()):\n",
    "    metadata.extend(create_protein_info(protein_name, protein_desc))\n",
    "\n",
    "pd.DataFrame(metadata, columns=[\"protein_name\", \"pubmed_id\", \"title\", \"authors\", \"mesh_terms\", \"paper_url\"]).to_csv(\"../background_information_data/protein_data/metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50c219d-d163-431f-9759-4607bb3bf7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import Entrez, Medline\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import signal\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Set email address (required for NCBI API usage)\n",
    "Entrez.email = 'sks6765@psu.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5a7b3-89d0-4240-af3c-513008ed620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_viz(dataframe, column1, column2, label=1):\n",
    "    # Creating an interactive graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    for index, row in dataframe.iterrows():\n",
    "        G.add_edge(row[column1], row[column2], label=row[label] if label != 1 else 1)\n",
    "    \n",
    "    # Add edge labels\n",
    "    edge_labels = {(u, v): d['label'] for u, v, d in G.edges(data=True)}\n",
    "    nx.set_edge_attributes(G, edge_labels, 'label')\n",
    "    \n",
    "    # Position nodes using spring layout\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Extract node and edge information\n",
    "    node_x = [pos[node][0] for node in G.nodes()]\n",
    "    node_y = [pos[node][1] for node in G.nodes()]\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_text = []\n",
    "    for edge in G.edges():\n",
    "        source, target = edge\n",
    "        x0, y0 = pos[source]\n",
    "        x1, y1 = pos[target]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "        edge_text.append(G.get_edge_data(*edge)['label'])  # Use existing edge labels\n",
    "    \n",
    "    # Create a Plotly figure\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        mode='lines',\n",
    "        line_shape='spline',\n",
    "        opacity=0.5,\n",
    "        hoverinfo='none'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        hovertext=[f'Node {node}' for node in G.nodes()]\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=[(pos[edge[0]][0] + pos[edge[1]][0]) / 2 for edge in G.edges()],\n",
    "        y=[(pos[edge[0]][1] + pos[edge[1]][1]) / 2 for edge in G.edges()],\n",
    "        mode='text',\n",
    "        text=edge_text,\n",
    "        textposition='middle center',\n",
    "        hoverinfo='none'\n",
    "    )])\n",
    "    \n",
    "    # Customize the layout\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        hovermode='x',\n",
    "        margin=dict(b=20, l=5, r=5, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0ce48-1932-4829-8fec-627c0134602f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Extra code - might use later. This is to retrieve info from the gene db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9351a87-3aed-44ee-b8a7-732e50be1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTES/OBSERVATIONS:\n",
    "\n",
    "1. I decided to do a_is_acting == 't' as otherwise, we would get matches like this which, imo, doesn't really make sense, i.e., \n",
    "what does a protein \"acting\" on a drug mean? - drug acting on a protein makes more sense.\n",
    "\n",
    "8063 \t9606.ENSP00000320025 \tCIDs00003333 \tinhibition \tinhibition \tf \t991\n",
    "8064 \tCIDs00003333 \t9606.ENSP00000320025 \tinhibition \tinhibition \tt \t991\n",
    "\n",
    "2. In theory, actions.query(\"(item_id_a in @l1) and (item_id_b in @l2)\") should return 100 rows for each interaction. However, there are \n",
    "certain dpi's like below that have different interaction scores, whether a acts on b, etc. Thus, we filter out those with our conditions.\n",
    "\n",
    "21666299 \tCIDs00060795 \t9606.ENSP00000289753 \tinhibition \tinhibition \tt \t958\n",
    "21666301 \tCIDs00060795 \t9606.ENSP00000289753 \tbinding \tNaN \tt \t800\n",
    "\n",
    "3. There are certain pairs like, ('CIDs00002435', '9606.ENSP00000398832') that do not have an entry in the database either as\n",
    "(item_id_a, item_id_b) or (item_id_b, item_id_a) - This in turn reduces the number of samples.\n",
    "\n",
    "4. I am bound to see more a_is_acting = f since it is the major element in the actions db by about 30 times more than t.\n",
    "a_is_acting\n",
    "f    21078378\n",
    "t      695113.\n",
    "\n",
    "5. Even if I consider a->b and b->a relationships, I'm getting certain useless pairs such as the following which doesn't really give me any\n",
    "additional information.\n",
    "1448204 \t9606.ENSP00000241256 \tCIDs00002477 \tbinding \tNaN \tf \t894\n",
    "1448205 \tCIDs00002477 \t9606.ENSP00000241256 \tbinding \tNaN \tf \t894\n",
    "\n",
    "6. This will return a lower number than the rows in r since there can be multiple relationships between the same drug-protein pair.\n",
    "r = actions.query(\"item_id_a in @l1\")\n",
    "r = r[r.apply(lambda x: (x[\"item_id_a\"], x[\"item_id_b\"]) in target_pairs, axis=1)]\n",
    "len(set([(x.item_id_a, x.item_id_b) for x in r.itertuples()]))\n",
    "\n",
    "7. Out of 200 possible (a,b) and (b,a) combinations, 154 exist in the database.\n",
    "r = actions.query(\"item_id_a in @l1 or item_id_b in @l1\")\n",
    "len(set(target_pairs).intersection(set([(x.item_id_a, x.item_id_b) for x in r.itertuples()]).union(set([(x.item_id_b, x.item_id_a) for x in r.itertuples()]))))\n",
    "\"\"\"\n",
    "# Previous code - not needed now.\n",
    "l1 = [x[0] for x in target_pairs]\n",
    "'''\n",
    "I'm only searching one column (item_id_a) with the CID's. All of my CID's WILL be present in that column, since each CID is mentioned once\n",
    "in item_id_a and another in item_id_b.\n",
    "'''\n",
    "r = actions.query(\"item_id_a in @l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9996c2a-129a-4b63-b689-313fa5b36a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_info(record):\n",
    "    return f\"{record[0]['Entrezgene_gene']['Gene-ref']['Gene-ref_locus']} ({record[0]['Entrezgene_gene']['Gene-ref']['Gene-ref_desc']})\",\\\n",
    "    record[0][\"Entrezgene_summary\"],\\\n",
    "    record[0][\"Entrezgene_prot\"]\n",
    "\n",
    "cleaned_gene_id = int(re.sub(\"9606.ENSP0*\", \"\", gene))\n",
    "Entrez.email = \"sample_email@example.org\" #Doesn't really matter\n",
    "handle = Entrez.efetch(db=\"gene\", id=cleaned_gene_id, rettype=\"gb\", retmode=\"xml\")\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "try:\n",
    "    all_information[\"gene name\"], all_information[\"gene summary\"], all_information[\"proteins from gene\"] = get_protein_info(record)\n",
    "except:\n",
    "    continue\n",
    "\n",
    "Entrez.email = \"sample_email@example.org\" #Doesn't really matter\n",
    "handle = Entrez.efetch(db=\"gene\", id=7448, rettype=\"gb\", retmode=\"xml\")\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "all_information = {}\n",
    "all_information[\"gene name\"] = f\"{record[0]['Entrezgene_gene']['Gene-ref']['Gene-ref_locus']} ({record[0]['Entrezgene_gene']['Gene-ref']['Gene-ref_desc']})\"\n",
    "all_information[\"gene summary\"] = record[0][\"Entrezgene_summary\"]\n",
    "all_information[\"proteins from gene\"] = record[0][\"Entrezgene_prot\"]\n",
    "all_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38b2ff-e4f7-48b9-9d5b-89419d13f298",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DDI sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126409d1-63e7-4d08-8111-e1a44efe4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi = pd.read_csv(\"bio-decagon-combo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd10e7-f24f-444e-a21a-d142bba60d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError\n",
    "\n",
    "def CID_information(CID: int, timeout: int = 5):\n",
    "\n",
    "    # Set the signal handler\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "    # Set the alarm\n",
    "    signal.alarm(timeout)\n",
    "\n",
    "    try:\n",
    "        cleaned_cid = int(re.sub(r\"CID[m|s]*0*\", \"\", CID))\n",
    "        compound = pcp.Compound.from_cid(cleaned_cid)\n",
    "        try:\n",
    "            compound_name = compound.synonyms[0]\n",
    "        except:\n",
    "            compound_name = compound.iupac_name\n",
    "        return (compound_name, compound.canonical_smiles)\n",
    "    except TimeoutError:\n",
    "        print(\"Timeout: Function took too long to complete.\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Disable the alarm\n",
    "        signal.alarm(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6597181-9986-4e05-8e1f-e2f6760f2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_ddis_to_sample = 10000\n",
    "number_of_ddis_to_retain = 300\n",
    "all_information = []\n",
    "sampled_ddis = ddi.sample(n = number_of_ddis_to_sample, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2f5ac-ece7-4356-829a-519cf0f2231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(all_information):\n",
    "    pd.DataFrame(all_information, columns=[\"drug_1_CID\", \"drug_1_name\", \"drug_1_SMILES\", \"relationship\", \"drug_2_CID\", \"drug_2_name\", \n",
    "                                               \"drug_2_SMILES\"]).to_csv(\"DDI_subset.csv\", index=False)\n",
    "\n",
    "for idx, sample in tqdm(enumerate(sampled_ddis.itertuples(index=False))):    \n",
    "    drug_1_CID = sample[0]\n",
    "    drug_2_CID = sample[1]\n",
    "    relationship = sample[3]\n",
    "\n",
    "    drug_1_info = CID_information(drug_1_CID)\n",
    "    drug_2_info = CID_information(drug_2_CID)\n",
    "    \n",
    "    try:\n",
    "        # I want the drug name to not include any numbers or brackets, i.e., only keep regular words.\n",
    "        if (not re.search(r\"[\\W\\d]|(sulfate)\", drug_1_info[0])) and (not re.search(r\"[\\W\\d]|(sulfate)\", drug_2_info[0])):\n",
    "\n",
    "            # This ensures that when I am retrieving the background information, I will get 20 pubmed hits.\n",
    "            drug1_query = f\"{drug_1_info[0]} AND hasabstract AND Humans AND (AD OR AE OR PK OR PD OR CO OR TU OR DE)\"\n",
    "            with Entrez.esearch(db='pubmed', term=drug1_query, retmax=20, sort=\"relevance\") as handle:\n",
    "                paper_list_drug1 = Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "            drug2_query = f\"{drug_2_info[0]} AND hasabstract AND Humans AND (AD OR AE OR PK OR PD OR CO OR TU OR DE)\"\n",
    "            with Entrez.esearch(db='pubmed', term=drug2_query, retmax=20, sort=\"relevance\") as handle:\n",
    "                paper_list_drug2 = Entrez.read(handle)[\"IdList\"]\n",
    "            \n",
    "            if len(paper_list_drug1) == 20 and len(paper_list_drug2) == 20:            \n",
    "                all_information.append((drug_1_CID, drug_1_info[0], drug_1_info[1], relationship, drug_2_CID, drug_2_info[0], drug_2_info[1]))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # timeout after 20 API calls.\n",
    "    if (idx % 20 == 0) and (idx != 0):\n",
    "        print(f\"Samples collected so far: {len(all_information)}. Saving checkpoint and cooling down ...\")\n",
    "        save_dataframe(all_information)\n",
    "        time.sleep(60)        \n",
    "    \n",
    "    if len(all_information) == number_of_ddis_to_retain:\n",
    "        print(\"Necessary number of DDIs obtained... exiting\")\n",
    "        save_dataframe(all_information)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc674f5-5335-4ec4-a642-03313518c635",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DPI sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7c95e-1baa-4e51-9862-72e73e972433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing actions dataframe\n",
    "\n",
    "actions = pd.read_csv(\"../base_data/9606.actions.v5.0.tsv\", sep=\"\\t\") #drug-protein pairs with labels\n",
    "actions = actions[actions['item_id_a'].str.startswith(\"CIDs\")]\n",
    "actions = actions.query(\"score > 900 and a_is_acting == 't'\").dropna() # Only retain \"strong\" interactions\n",
    "actions[\"item_id_a\"] = actions[\"item_id_a\"].str.replace(\"CIDs\", \"CID0\")\n",
    "actions.to_csv(\"action_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f695d8d-6edc-4421-94c7-6690101d9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_subset = pd.read_csv(\"DDI_subset.csv\")\n",
    "dpi = pd.read_csv(\"bio-decagon-targets-all.csv\")\n",
    "actions_subset = pd.read_csv(\"action_subset.csv\")\n",
    "\n",
    "all_drugs_CID = set(ddi_subset[\"drug_1_CID\"].to_list() + ddi_subset[\"drug_2_CID\"].to_list())\n",
    "\n",
    "# The query returns A LOT of DPIs. We take only a subset of them.\n",
    "dpi_subset = dpi.query(\"STITCH in @all_drugs_CID\").sample(n=5000, random_state=1)\n",
    "\n",
    "# Converting PubChem CID to STITCH CIDs (singular element) (https://www.biostars.org/p/155342/)\n",
    "dpi_subset['STITCH'] = dpi_subset['STITCH'].str.replace('CID0', 'CIDs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c75c4-6fdb-471c-b5c6-16d509ef4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = []\n",
    "for row in tqdm(dpi_subset.itertuples()):  \n",
    "    # Retrieving protein data from the given gene ID\n",
    "    url = f\"https://string-db.org/api/json/get_string_ids?identifiers={row.Gene}&species=9606\"\n",
    "    try:\n",
    "        response = requests.get(url).json()[0]\n",
    "        string_id = (response[\"stringId\"], response[\"preferredName\"], response[\"annotation\"], row.Gene)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    search_list.append((row.STITCH, string_id))\n",
    "\n",
    "target_pairs = list(zip([x[0] for x in search_list], [x[1][0] for x in search_list]))\n",
    "'''\n",
    "1. I can do the following check since if (a, b) does not exist in the db, (b, a) also wont.\n",
    "'''\n",
    "r = actions_subset[actions_subset.apply(lambda x: (x[\"item_id_a\"], x[\"item_id_b\"]) in set(target_pairs), axis=1)]\n",
    "\n",
    "search_df = pd.DataFrame({\"cid\": [x[0] for x in search_list], \"protein_number\": [x[1][0] for x in search_list], \n",
    "                          \"protein_name\": [x[1][1] for x in search_list], \"protein_desc\": [x[1][2] for x in search_list],\n",
    "                          \"gene\": [x[1][3] for x in search_list]})\n",
    "\n",
    "r = pd.merge(r, search_df, left_on=[\"item_id_a\", \"item_id_b\"], right_on=[\"cid\", \"protein_number\"], \n",
    "             how=\"inner\").drop(columns=[\"item_id_a\", \"item_id_b\", \"a_is_acting\", \"score\"]).drop_duplicates() # Need to drop duplicates because of \"a is acting\" column. That is, the same entry can be repeated twice because of \"a is acting\"\n",
    "\n",
    "r[\"cid\"] = r[\"cid\"].str.replace(\"CIDs\", \"CID0\") # Going back to PubChem CID to be compatible with other DDI\n",
    "r.to_csv(\"DPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ecf67-e248-4ce5-88d6-11b6a17efac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pairs = list(zip([x[0] for x in search_list], [x[1][0] for x in search_list]))\n",
    "'''\n",
    "1. I can do the following check since if (a, b) does not exist in the db, (b, a) also wont.\n",
    "'''\n",
    "r = actions_subset[actions_subset.apply(lambda x: (x[\"item_id_a\"], x[\"item_id_b\"]) in set(target_pairs), axis=1)]\n",
    "\n",
    "search_df = pd.DataFrame({\"cid\": [x[0] for x in search_list], \"protein_number\": [x[1][0] for x in search_list], \n",
    "                          \"protein_name\": [x[1][1] for x in search_list], \"protein_desc\": [x[1][2] for x in search_list],\n",
    "                          \"gene\": [x[1][3] for x in search_list]})\n",
    "\n",
    "r = pd.merge(r, search_df, left_on=[\"item_id_a\", \"item_id_b\"], right_on=[\"cid\", \"protein_number\"], \n",
    "             how=\"inner\").drop(columns=[\"item_id_a\", \"item_id_b\", \"a_is_acting\", \"score\"]).drop_duplicates() # Need to drop duplicates because of \"a is acting\" column. That is, the same entry can be repeated twice because of \"a is acting\"\n",
    "\n",
    "r[\"cid\"] = r[\"cid\"].str.replace(\"CIDs\", \"CID0\") # Going back to PubChem CID to be compatible with other DDI\n",
    "r.to_csv(\"DPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0d704-dd58-4e2a-8ab6-a9f186ceb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the drug name to the DPI dataframe for easy access\n",
    "ddi_subset = pd.read_csv(\"DDI_subset.csv\")\n",
    "dpi_subset = pd.read_csv(\"DPI_subset.csv\")\n",
    "\n",
    "d1 = pd.Series(ddi_subset.drug_1_name.values,index=ddi_subset.drug_1_CID).to_dict()\n",
    "d2 = pd.Series(ddi_subset.drug_2_name.values,index=ddi_subset.drug_2_CID).to_dict()\n",
    "all_drug_cid_name = {**d1, **d2}\n",
    "\n",
    "all_drug_cid_name_df = pd.DataFrame(all_drug_cid_name.items(), columns=[\"cid\", \"drug_name\"])\n",
    "dpi_subset = pd.merge(dpi_subset, all_drug_cid_name_df, left_on=[\"cid\"], right_on=[\"cid\"])\n",
    "dpi_subset.to_csv(\"DPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867a1cf-1a96-40fd-abc6-5cbdea5a4ca2",
   "metadata": {},
   "source": [
    "## PPI sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce8f1e-6073-4025-a858-7ecc57b20862",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi = pd.read_csv(\"bio-decagon-ppi.csv\")\n",
    "dpi_subset = pd.read_csv(\"DPI_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad733780-8bec-46c0-be35-c06a5c2b7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes = set(dpi_subset[\"gene\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05701670-88c7-498f-9898-27acfeb7cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this now to have more number of proteins.\n",
    "r = ppi.query(\"`Gene 1` in @all_genes or `Gene 2` in @all_genes\").sample(n=500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f362858-eb13-4bd6-8928-f28dc311577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use this condition since I don't want to introduce new genes as a part of the dataset. - not using this anymore - getting much less proteins\n",
    "r = ppi[ppi[\"Gene 1\"].isin(all_genes) & ppi[\"Gene 2\"].isin(all_genes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf922b-e522-4950-a489-8c0c7ffb1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nscore - neighborhood score, (computed from the inter-gene nucleotide count).\n",
    "fscore - fusion score (derived from fused proteins in other species).\n",
    "pscore - cooccurence score of the phyletic profile (derived from similar absence/presence patterns of genes).\n",
    "ascore - coexpression score (derived from similar pattern of mRNA expression measured by DNA arrays and similar technologies).\n",
    "escore - experimental score (derived from experimental data, such as, affinity chromatography).\n",
    "dscore - database score (derived from curated data of various databases).\n",
    "tscore - textmining score (derived from the co-occurrence of gene/protein names in abstracts).\n",
    "'''\n",
    "all_res = []\n",
    "for row in tqdm(r.itertuples()):    \n",
    "    # I'm passing the gene ID directly since STRING maps these ID's automatically to the gene's main protein.\n",
    "    # species 9606 = human.\n",
    "    url = f\"https://string-db.org/api/json/network?identifiers={'%0d'.join([str(row._1), str(row._2)])}&species=9606\"\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        response[0][\"Gene 1\"] = row._1\n",
    "        response[0][\"Gene 2\"] = row._2\n",
    "        all_res.append(response[0])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bdd50-aa25-4e90-8a92-ee4325fc9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "My interpretation of the scores (after reading a bit online):\n",
    "nscore - How close the 2 genes are in the genome (the entire set of organism DNA)\n",
    "fscore - Evidence of whether the 2 genes fuse together to form a hybrid gene\n",
    "pscore - Evidence of whether the 2 genes have a common ancestor (homologous)\n",
    "ascore - Evidence of whether the 2 genes are simultaneously activated or not.\n",
    "escore - Evidence of whether the 2 genes have been shown to interact through lab experiments.\n",
    "dscore - Degree of presence in existing datasbases.\n",
    "tscore - Evidence of reporting in literature.\n",
    "'''\n",
    "def return_edge_label(pair: dict):\n",
    "    pair_scores = {'nscore': pair['nscore'], 'fscore': pair['fscore'], 'pscore': pair['pscore'], 'ascore': pair['ascore'], \n",
    "                   'escore': pair['escore'], 'dscore': pair['dscore'], 'tscore': pair['tscore']}\n",
    "    best_evidence = max(pair_scores.items(), key=lambda x: x[1])\n",
    "    if best_evidence[0] == 'nscore':\n",
    "        return \"neighborhood_evidence\"\n",
    "    elif best_evidence[0] == 'fscore':\n",
    "        return \"fusion_evidence\"\n",
    "    elif best_evidence[0] == 'pscore':\n",
    "        return \"homologous_evidence\"\n",
    "    elif best_evidence[0] == 'ascore':\n",
    "        return \"co-expression_evidence\"\n",
    "    elif best_evidence[0] == 'escore':\n",
    "        return \"experimental_evidence\"\n",
    "    elif best_evidence[0] == 'dscore':\n",
    "        return \"database_evidence\"\n",
    "    elif best_evidence[0] == 'tscore':\n",
    "        return \"literature_evidence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a15c36-08d7-4a18-ac45-d17ca59c35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(all_res)):\n",
    "    all_res[idx][\"relationship\"] = return_edge_label(all_res[idx])\n",
    "\n",
    "pd.DataFrame(all_res)[[\"Gene 1\", \"Gene 2\", \"stringId_A\", \"stringId_B\", \"preferredName_A\", \"preferredName_B\", \n",
    "                       \"relationship\"]].to_csv(\"PPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eac10b-12f3-4995-9273-b2f41eb04bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding annotation data for new proteins\n",
    "\n",
    "ppi_subset_genes = set(ppi_subset[\"Gene 1\"].to_list()).union(set(ppi_subset[\"Gene 1\"].to_list()))\n",
    "\n",
    "def get_protein_annotation(gene_number):\n",
    "    url = f\"https://string-db.org/api/json/get_string_ids?identifiers={gene_number}&species=9606\"\n",
    "    try:\n",
    "        return requests.get(url).json()[0][\"annotation\"]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "gene_annotations = {}\n",
    "for row in tqdm(ppi_subset.itertuples(index=False)): \n",
    "    gene_1 = row[0]\n",
    "    if gene_1 not in gene_annotations:\n",
    "        gene_annotations[gene_1] = get_protein_annotation(gene_1)\n",
    "\n",
    "    gene_2 = row[1]\n",
    "    if gene_2 not in gene_annotations:\n",
    "        gene_annotations[gene_2] = get_protein_annotation(gene_2)\n",
    "\n",
    "gene_1_annotations = []\n",
    "gene_2_annotations = []\n",
    "for row in ppi_subset.itertuples(index=False):\n",
    "    gene_1_annotations.append(gene_annotations[row[0]])\n",
    "    gene_2_annotations.append(gene_annotations[row[1]])\n",
    "\n",
    "ppi_subset[\"protein_1_desc\"] = gene_1_annotations\n",
    "ppi_subset[\"protein_2_desc\"] = gene_2_annotations\n",
    "\n",
    "ppi_subset.to_csv(\"PPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6fe92-1d60-4aa7-881c-9f6a7e29374d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Collecting Background information\n",
    "\n",
    "Keywords obtained from https://pubmed.ncbi.nlm.nih.gov/help/#proximity-searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd86cc9-363c-4c9f-9a17-244e1fe6e70b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Drug info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068673c-c0ca-4784-be7b-01058b8778c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_subset = pd.read_csv(\"DDI_subset.csv\")\n",
    "all_drugs = set(ddi_subset[\"drug_1_name\"].to_list() + ddi_subset[\"drug_2_name\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd1921-86bd-4c3b-97c2-3944a2dfb5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drug_info(drug: str):\n",
    "    '''\n",
    "    AD - Administration and Dosage\n",
    "    AE - Adverse Effects\n",
    "    PK - Pharmacokinetics\n",
    "    PD - Pharmacology\n",
    "    CO - Complications\n",
    "    TU - Therapeutic Use\n",
    "    DE - Drug Effects\n",
    "    '''\n",
    "    drug_query = f\"{drug} AND hasabstract AND Humans AND (AD OR AE OR PK OR PD OR CO OR TU OR DE)\"\n",
    "\n",
    "    # Getting relevant papers for given drug\n",
    "    with Entrez.esearch(db='pubmed', term=drug_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = \"\"\n",
    "    for paper_id, record in zip(paper_list, record_list):\n",
    "        try:\n",
    "            metadata.append((drug, paper_id, record[\"TI\"], record[\"AU\"], record[\"MH\"], f\"https://pubmed.ncbi.nlm.nih.gov/{paper_id}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with Path(f\"background_information_data/drug_data/{drug}.txt\").open(\"w\") as output_file:\n",
    "        output_file.write(all_abstracts_string)\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb443a-79ff-435c-8189-37d99c31161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing drug SMILES data\n",
    "for row in ddi_subset.itertuples():\n",
    "    drug1_path = Path(f\"background_information_data/drug_data/{row.drug_1_name}_SMILES.txt\")\n",
    "    if drug1_path.exists() is False:\n",
    "        with drug1_path.open(\"w\") as out_file:\n",
    "            out_file.write(row.drug_1_SMILES)\n",
    "\n",
    "    drug2_path = Path(f\"background_information_data/drug_data/{row.drug_2_name}_SMILES.txt\")\n",
    "    if drug2_path.exists() is False:\n",
    "        with drug2_path.open(\"w\") as out_file:\n",
    "            out_file.write(row.drug_2_SMILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99714a2-320f-4515-8276-ad0f125f8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all the relevant information \n",
    "metadata = []\n",
    "for drug in tqdm(all_drugs):\n",
    "    metadata.extend(create_drug_info(drug))\n",
    "\n",
    "pd.DataFrame(metadata, columns=[\"drug_name\", \"pubmed_id\", \"title\", \"authors\", \"mesh_terms\", \"paper_url\"]).to_csv(\"background_information_data/drug_data/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ccee1-296e-4832-9589-eec192b558b0",
   "metadata": {},
   "source": [
    "### Protein info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5547d-a192-4d76-afc4-74c942fdc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi_subset = pd.read_csv(\"DPI_subset.csv\")\n",
    "ppi_subset = pd.read_csv(\"PPI_subset.csv\")\n",
    "\n",
    "dpi_proteins = dpi_subset[[\"protein_name\", \"protein_desc\"]].drop_duplicates().set_index(\"protein_name\")[\"protein_desc\"].to_dict()\n",
    "\n",
    "ppi_proteins_1 = ppi_subset[[\"preferredName_A\", \"protein_1_desc\"]].drop_duplicates().set_index(\"preferredName_A\")[\"protein_1_desc\"].to_dict()\n",
    "ppi_proteins_2 = ppi_subset[[\"preferredName_B\", \"protein_2_desc\"]].drop_duplicates().set_index(\"preferredName_B\")[\"protein_2_desc\"].to_dict()\n",
    "ppi_proteins = {**ppi_proteins_1, **ppi_proteins_2}\n",
    "\n",
    "all_proteins = {**dpi_proteins, **ppi_proteins}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621eb77-bc9d-43c8-b634-58e1544fa2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_protein_info(protein: str, protein_desc: str):\n",
    "    '''\n",
    "    CH - chemistry\n",
    "    ME - metabolism\n",
    "    PH - physiology\n",
    "    GE - genetics\n",
    "    '''\n",
    "    protein_query = f\"{protein} AND hasabstract AND Humans AND (CH OR ME OR PH OR GE)\"\n",
    "\n",
    "    # Getting relevant papers for given protein\n",
    "    with Entrez.esearch(db='pubmed', term=protein_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = protein_desc + \"\\n\" # Adding the annotation information from STRING\n",
    "    for paper_id, record in zip(paper_list, record_list):\n",
    "        try:\n",
    "            metadata.append((protein, paper_id, record[\"TI\"], record[\"AU\"], record[\"MH\"], f\"https://pubmed.ncbi.nlm.nih.gov/{paper_id}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with Path(f\"background_information_data/protein_data/{protein}.txt\").open(\"w\") as output_file:\n",
    "        output_file.write(all_abstracts_string)\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ee013-44ef-44ac-b3fe-b2ef7e524688",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for protein_name, protein_desc in tqdm(all_proteins.items()):\n",
    "    metadata.extend(create_protein_info(protein_name, protein_desc))\n",
    "\n",
    "pd.DataFrame(metadata, columns=[\"protein_name\", \"pubmed_id\", \"title\", \"authors\", \"mesh_terms\", \"paper_url\"]).to_csv(\"background_information_data/protein_data/metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

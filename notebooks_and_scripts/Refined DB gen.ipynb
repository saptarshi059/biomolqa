{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47838ed0-fbf9-479d-be25-8396416e5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "from chemspipy import ChemSpider\n",
    "from Bio import Entrez, Medline\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import requests\n",
    "import signal\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Set email address (required for NCBI API usage)\n",
    "Entrez.email = 'sks6765@psu.edu'\n",
    "\n",
    "# Chemspider API key\n",
    "cs = ChemSpider('AoAVRxmbgZ1ZSsk4Zlbua1jev4EWDFSI7XB2U19B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea01064-aeba-438b-b03d-260fe35fc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_viz(dataframe, column1, column2, label=1):\n",
    "    # Creating an interactive graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    for index, row in dataframe.iterrows():\n",
    "        G.add_edge(row[column1], row[column2], label=row[label] if label != 1 else 1)\n",
    "    \n",
    "    # Add edge labels\n",
    "    edge_labels = {(u, v): d['label'] for u, v, d in G.edges(data=True)}\n",
    "    nx.set_edge_attributes(G, edge_labels, 'label')\n",
    "    \n",
    "    # Position nodes using spring layout\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Extract node and edge information\n",
    "    node_x = [pos[node][0] for node in G.nodes()]\n",
    "    node_y = [pos[node][1] for node in G.nodes()]\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_text = []\n",
    "    for edge in G.edges():\n",
    "        source, target = edge\n",
    "        x0, y0 = pos[source]\n",
    "        x1, y1 = pos[target]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "        edge_text.append(G.get_edge_data(*edge)['label'])  # Use existing edge labels\n",
    "    \n",
    "    # Create a Plotly figure\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        mode='lines',\n",
    "        line_shape='spline',\n",
    "        opacity=0.5,\n",
    "        hoverinfo='none'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        hovertext=[f'Node {node}' for node in G.nodes()]\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=[(pos[edge[0]][0] + pos[edge[1]][0]) / 2 for edge in G.edges()],\n",
    "        y=[(pos[edge[0]][1] + pos[edge[1]][1]) / 2 for edge in G.edges()],\n",
    "        mode='text',\n",
    "        text=edge_text,\n",
    "        textposition='middle center',\n",
    "        hoverinfo='none'\n",
    "    )])\n",
    "    \n",
    "    # Customize the layout\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        hovermode='x',\n",
    "        margin=dict(b=20, l=5, r=5, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb589ef-f268-4be7-81fd-dabe969a12d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DDI Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb7bbc-35d7-4602-95ec-987c7ce468f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cas_number(cas_number):\n",
    "    url = f\"https://commonchemistry.cas.org/api/detail?cas_rn={cas_number}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()[\"name\"]\n",
    "        if data:\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def CID_information(CID: int):\n",
    "    cleaned_cid = int(re.sub(r\"CID[m|s]*0*\", \"\", CID))\n",
    "    compound = pcp.Compound.from_cid(cleaned_cid)\n",
    "    \n",
    "    try:\n",
    "        compound_name = cs.get_compound(cs.filter_results(cs.filter_inchikey(compound.inchikey))[0]).common_name\n",
    "    except:\n",
    "        if compound.synonyms:\n",
    "            flag = 0\n",
    "            for name in compound.synonyms:\n",
    "                if not re.search(r\"^[a-zA-Z\\s]+$\", name):\n",
    "                    flag = 1\n",
    "                    compound_name = name\n",
    "                    break\n",
    "\n",
    "            if flag == 0:\n",
    "                compound_name = compound.synonyms[0]\n",
    "        \n",
    "                # Checking if the returned name is a valid CAS number\n",
    "                if re.search(r\"\\b\\d{2,7}-\\d{2}-\\d\\b\", compound_name):\n",
    "                    cas_name = search_cas_number(compound_name)\n",
    "                    if (cas_name is not None) and (re.search(r\"^[a-zA-Z\\s]+$\", cas_name)):\n",
    "                        compound_name = cas_name\n",
    "        else:\n",
    "            compound_name = compound.iupac_name\n",
    "\n",
    "    return (compound_name, compound.canonical_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2932d4a-4fc7-40a4-b485-282ff9b91cec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run this cell only once! - Create and save the deterministic subgraph dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41a15e-93ba-4add-90c6-d568f3fd4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi = pd.read_csv(\"../data/base_data/bio-decagon-combo.csv\")\n",
    "\n",
    "G = nx.from_pandas_edgelist(ddi, 'STITCH 1', 'STITCH 2')\n",
    "\n",
    "# Get the largest connected component\n",
    "largest_component = max(nx.connected_components(G), key=len)\n",
    "\n",
    "largest_component.remove(\"CID006398525\") # This node has NO synonym, common name, chemspider entry, etc.\n",
    "\n",
    "# Sample nodes from the largest component\n",
    "sample_size = 500\n",
    "sample_nodes = np.random.choice(list(largest_component), size=sample_size, replace=False)\n",
    "\n",
    "# Get the subgraph induced by the sampled nodes\n",
    "subgraph = G.subgraph(sample_nodes)\n",
    "\n",
    "# Convert the subgraph back to a DataFrame\n",
    "sampled_df = nx.to_pandas_edgelist(subgraph)\n",
    "\n",
    "sampled_df.rename(columns={\"source\": \"STITCH 1\", \"target\": \"STITCH 2\"}, inplace=True)\n",
    "\n",
    "# Merging the sampled df with base ddi to get the side effect names\n",
    "sampled_df = pd.merge(sampled_df, ddi).drop_duplicates(subset=[\"STITCH 1\", \"STITCH 2\"])\n",
    "\n",
    "sampled_df.to_csv(\"../data/mined_data/sampled_ddi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc8eaf-e5ef-4c18-8fdf-d974ab2653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check the output\n",
    "\n",
    "# Visualizing sampled_df as a graph to see that there are no disconnected edges\n",
    "create_graph_viz(sampled_df, \"STITCH 1\", \"STITCH 2\")\n",
    "\n",
    "all_drugs_according_to_df = len(set(sampled_df['STITCH 1'].to_list()).union(set(sampled_df['STITCH 2'].to_list())))\n",
    "\n",
    "# Verify that you get the same value from the sampled_df as a graph\n",
    "G = nx.from_pandas_edgelist(sampled_df, 'STITCH 1', 'STITCH 2')\n",
    "number_of_drug_nodes = len(G.nodes())\n",
    "\n",
    "assert all_drugs_according_to_df == number_of_drug_nodes\n",
    "\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4b83f-cf73-47c1-825a-e6c619409007",
   "metadata": {},
   "source": [
    "## Run these cells to get the drug info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d22710-a8f4-49ff-994f-e799b66b65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = pd.read_csv(\"../data/mined_data/sampled_ddi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109eb20-1e03-4d05-b0d6-56951c391fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drugs = set(sampled_df[\"STITCH 1\"].unique()).union(set(sampled_df[\"STITCH 2\"].unique()))\n",
    "drug_info = {}\n",
    "for drug_cid in tqdm(all_drugs):\n",
    "    drug_info[drug_cid] = CID_information(drug_cid)\n",
    "\n",
    "# Saving so that I don't have to requery API (rate limits)\n",
    "with Path(\"../data/mined_data/drug_annotations.pkl\").open(\"wb\") as f:\n",
    "    pickle.dump(drug_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f3b79-4515-4a1d-b841-a9fa14cbde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"../data/mined_data/drug_annotations.pkl\").open(\"rb\") as f:\n",
    "    drug_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa77aa-279b-455f-a17e-7328ee06e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_rows = []\n",
    "for row in tqdm(sampled_df.itertuples(index=False)):\n",
    "    drug_1_CID = row[0]\n",
    "    drug_2_CID = row[1]\n",
    "    relationship = row[3]\n",
    "\n",
    "    drug_1_info = drug_info[drug_1_CID]\n",
    "    drug_2_info = drug_info[drug_2_CID]\n",
    "    \n",
    "    drug_1_name = drug_1_info[0]\n",
    "    drug_1_SMILES = drug_1_info[1]\n",
    "\n",
    "    drug_2_name = drug_2_info[0]\n",
    "    drug_2_SMILES = drug_2_info[1]\n",
    "\n",
    "    enhanced_rows.append((drug_1_CID, drug_1_name, drug_1_SMILES, relationship, drug_2_CID, drug_2_name, drug_2_SMILES))\n",
    "\n",
    "pd.DataFrame(data=enhanced_rows, \n",
    "             columns=[\"drug_1_CID\", \"drug_1_name\", \"drug_1_SMILES\", \"relationship\",\n",
    "                      \"drug_2_CID\", \"drug_2_name\", \"drug_2_SMILES\"]).to_csv(\"../data/mined_data/DDI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecf766-2dc6-4c8a-9c41-96070eb78024",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DPI Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8250a4-1864-458f-97de-6ec4d642f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the DPIs and their annotations\n",
    "\n",
    "dpi = pd.read_csv(\"../data/base_data/bio-decagon-targets-all.csv\")\n",
    "ddi_subset = pd.read_csv(\"../data/mined_data/DDI_subset.csv\")\n",
    "\n",
    "all_drugs = np.unique(np.concatenate((ddi_subset[\"drug_1_CID\"].unique(), ddi_subset[\"drug_2_CID\"].unique())))\n",
    "\n",
    "# This will return less number of drugs since NOT ALL DRUGS HAVE DPIS!\n",
    "dpis_needed = dpi.query(\"STITCH in @all_drugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d3ce6-93ca-4574-84eb-5d56c90d479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Genes to proteins\n",
    "unique_genes = dpis_needed[\"Gene\"].unique()\n",
    "gene_protein = {}\n",
    "for gene in tqdm(unique_genes):  \n",
    "    # Retrieving protein data from the given gene ID\n",
    "    url = f\"https://string-db.org/api/json/get_string_ids?identifiers={gene}&species=9606\"\n",
    "    try:\n",
    "        response = requests.get(url).json()[0]\n",
    "        gene_protein[gene] = (response[\"stringId\"], response[\"preferredName\"], response[\"annotation\"])  \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "with Path(\"../data/mined_data/protein_annotations.pkl\").open(\"wb\") as f:\n",
    "    pickle.dump(gene_protein, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca143fe-74ba-4771-aee3-3bfb915b815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"../data/mined_data/protein_annotations.pkl\").open(\"rb\") as f:\n",
    "    gene_protein = pickle.load(f)\n",
    "\n",
    "gene_protein_df = pd.DataFrame.from_dict(gene_protein, orient=\"index\").reset_index()\n",
    "gene_protein_df.rename(columns={\"index\": \"Gene\", 0: \"stringID\", 1: \"protein_name\", 2:\"protein_desc\"}, inplace=True)\n",
    "annotated_dpis = pd.merge(dpis_needed, gene_protein_df)\n",
    "annotated_dpis.rename(columns={\"STITCH\": \"item_id_a\", \"stringID\": \"item_id_b\"}, inplace=True)\n",
    "\n",
    "actions_subset = pd.read_csv(\"../data/base_data/actions.csv\")\n",
    "merged = pd.merge(annotated_dpis, actions_subset)\n",
    "merged.rename(columns={\"item_id_a\": \"cid\", \"item_id_b\": \"stringId\"}, inplace=True)\n",
    "\n",
    "# Adding drug names to the DPI dataframe for completeness\n",
    "d1_names = ddi_subset[[\"drug_1_CID\", \"drug_1_name\"]].set_index(\"drug_1_CID\")[\"drug_1_name\"].to_dict()\n",
    "d2_names = ddi_subset[[\"drug_2_CID\", \"drug_2_name\"]].set_index(\"drug_2_CID\")[\"drug_2_name\"].to_dict()\n",
    "drug_cid_df = pd.DataFrame({**d1_names, **d2_names}.items(), columns=[\"cid\", \"drug_name\"])\n",
    "\n",
    "pd.merge(merged, drug_cid_df).drop_duplicates().to_csv(\"../data/mined_data/DPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9464f-4901-4e65-a0c1-8609f223b394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Collecting Background information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7a97b-48d3-4bec-937a-ca7827f57abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_subset = pd.read_csv(\"../data/mined_data/DDI_subset.csv\")\n",
    "dpi_subset = pd.read_csv(\"../data/mined_data/DPI_subset.csv\")\n",
    "\n",
    "all_drugs = set(ddi_subset[\"drug_1_name\"].unique()).union(set(ddi_subset[\"drug_2_name\"].unique()))\n",
    "\n",
    "'''\n",
    "Had to filter the drugs since wikipedia was erroneoulsy mapping some identifiers which are PubChem specifc such as 12080-13-9, etc.\n",
    "Better to thus stick to commercial named drugs with a lot of info.\n",
    "'''\n",
    "filtered_drugs = list(filter(lambda x: re.search(r\"^[a-zA-Z\\s]+$\", x), all_drugs))\n",
    "\n",
    "all_proteins = dpi_subset[[\"protein_name\", \"protein_desc\"]].set_index(\"protein_name\")[\"protein_desc\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203d8ef-0d54-495f-8ce6-a5decc4abf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for reference sections and Wikipedia search parameters\n",
    "REFERENCE_SECTIONS = [\"== References ==\", \"== See also ==\", \"== Further reading ==\", \"== External links ==\"]\n",
    "SEARCH_RESULTS = 1\n",
    "\n",
    "def clean_doc(doc):\n",
    "    \"\"\"Clean and extract relevant content from a Wikipedia page.\"\"\"\n",
    "    return re.split(\"|\".join(REFERENCE_SECTIONS), doc)[0].strip()\n",
    "\n",
    "def get_wikipedia_content(entity):\n",
    "    try:\n",
    "        page_title = wikipedia.search(entity, results=SEARCH_RESULTS)[0]\n",
    "        page_content = wikipedia.page(page_title, auto_suggest=False).content\n",
    "        return clean_doc(page_content)\n",
    "    except (IndexError, wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError) as e:\n",
    "        return None\n",
    "\n",
    "def create_drug_info(drug: str):\n",
    "    '''\n",
    "    AD - Administration and Dosage\n",
    "    AE - Adverse Effects\n",
    "    PK - Pharmacokinetics\n",
    "    PD - Pharmacology\n",
    "    CO - Complications\n",
    "    TU - Therapeutic Use\n",
    "    DE - Drug Effects\n",
    "    '''\n",
    "    drug_query = f\"({drug}[TI]) AND (hasabstract) AND (english[la]) AND (medline[sb]) AND ({drug}[AD] OR \\\n",
    "    {drug}[AE] OR {drug}[PK] OR {drug}[PD] OR {drug}[CO] OR {drug}[TU] OR {drug}[DE] OR {drug}[TO] OR {drug}[CT] \\\n",
    "    OR {drug}[DT] OR {drug}[PO])\"\n",
    "\n",
    "    # Getting relevant papers for given drug\n",
    "    with Entrez.esearch(db='pubmed', term=drug_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "\n",
    "    if paper_list == []:\n",
    "        return None\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    filtered_record_list = list(filter(lambda rec: \"MH\" in rec and not any(re.match(\"animals*\", x, re.IGNORECASE) for x in rec[\"MH\"]), record_list))\n",
    "\n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = \"\"\n",
    "    for record in filtered_record_list:\n",
    "        try:\n",
    "            metadata.append((drug, record[\"PMID\"], record[\"TI\"], record[\"AU\"], record[\"MH\"], \n",
    "                             f\"https://pubmed.ncbi.nlm.nih.gov/{record[\"PMID\"]}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return (all_abstracts_string, metadata)\n",
    "\n",
    "def create_protein_info(protein: str, protein_desc: str):\n",
    "    '''\n",
    "    CH - chemistry\n",
    "    ME - metabolism\n",
    "    PH - physiology\n",
    "    GE - genetics\n",
    "    AN - Analysis\n",
    "    BI - Biosynthesis\n",
    "    CS - Chemical Synthesis\n",
    "    DF - Deficiency\n",
    "    '''\n",
    "    protein_query = f\"({protein}[TI]) AND (hasabstract) AND (english[la]) AND (medline[sb]) AND ({protein}[CH] OR \\\n",
    "    {protein}[ME] OR {protein}[PH] OR {protein}[GE] OR {protein}[AN] OR {protein}[BI] OR {protein}[CS] OR {protein}[DF])\"\n",
    "\n",
    "    # Getting relevant papers for given protein\n",
    "    with Entrez.esearch(db='pubmed', term=protein_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "\n",
    "    if paper_list == []:\n",
    "        return None\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    filtered_record_list = list(filter(lambda rec: \"MH\" in rec and not any(re.match(\"animals*\", x, re.IGNORECASE) for x in rec[\"MH\"]), record_list))\n",
    "    \n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = protein_desc + \"\\n\" # Adding the annotation information from STRING\n",
    "    for record in filtered_record_list:\n",
    "        try:\n",
    "            metadata.append((protein, record[\"PMID\"], record[\"TI\"], record[\"AU\"], record[\"MH\"], \n",
    "                             f\"https://pubmed.ncbi.nlm.nih.gov/{record[\"PMID\"]}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return (all_abstracts_string, metadata)\n",
    "\n",
    "def write_content(entity, content, entity_type, source):\n",
    "    file_path = Path(f\"../data/background_information_data/{entity_type}_data/{source}/{entity}.txt\")\n",
    "    try:\n",
    "        with file_path.open(\"w\") as f:\n",
    "            f.write(content)\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file for {drug}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadd0b9-1ef3-4af2-8202-0389dc12ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for idx, drug in tqdm(enumerate(filtered_drugs)):\n",
    "    wiki_content = get_wikipedia_content(drug)      \n",
    "    pubmed_content = create_drug_info(drug)\n",
    "    if wiki_content is not None:\n",
    "        write_content(drug, wiki_content, \"drug\", \"Wiki\")\n",
    "    if pubmed_content is not None:\n",
    "        write_content(drug, pubmed_content[0], \"drug\", \"PubMed\")\n",
    "        metadata.extend(pubmed_content[1])\n",
    "    if (idx % 10 == 0) and (idx != 0):\n",
    "        time.sleep(5)\n",
    "pd.DataFrame(metadata, columns=[\"drug_name\", \"pubmed_id\", \"title\", \"authors\", \n",
    "                                \"mesh_terms\", \"paper_url\"]).to_csv(\"../data/background_information_data/drug_data/PubMed/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049272d-cf78-46e9-ac8a-bb66a116f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for idx, (protein_name, protein_desc) in tqdm(enumerate(all_proteins.items())):\n",
    "    wiki_content = get_wikipedia_content(protein_name)\n",
    "    pubmed_content = create_protein_info(protein_name, protein_desc) \n",
    "    if wiki_content is not None:\n",
    "        write_content(protein_name, wiki_content, \"protein\", \"Wiki\")\n",
    "    if pubmed_content is not None:\n",
    "        write_content(protein_name, pubmed_content[0], \"protein\", \"PubMed\")\n",
    "        metadata.extend(pubmed_content[1])\n",
    "    if (idx % 10 == 0) and (idx != 0):\n",
    "        time.sleep(5)\n",
    "pd.DataFrame(metadata, columns=[\"protein_name\", \"pubmed_id\", \"title\", \"authors\", \n",
    "                                \"mesh_terms\", \"paper_url\"]).to_csv(\"../data/background_information_data/protein_data/PubMed/metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47838ed0-fbf9-479d-be25-8396416e5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "from chemspipy import ChemSpider\n",
    "from Bio import Entrez, Medline\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import requests\n",
    "import signal\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Set email address (required for NCBI API usage)\n",
    "Entrez.email = 'sks6765@psu.edu'\n",
    "\n",
    "# Chemspider API key\n",
    "cs = ChemSpider(os.getenv(\"CHEMSPIDER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea01064-aeba-438b-b03d-260fe35fc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_viz(dataframe, column1, column2, label=1):\n",
    "    # Creating an interactive graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    for index, row in dataframe.iterrows():\n",
    "        G.add_edge(row[column1], row[column2], label=row[label] if label != 1 else 1)\n",
    "    \n",
    "    # Add edge labels\n",
    "    edge_labels = {(u, v): d['label'] for u, v, d in G.edges(data=True)}\n",
    "    nx.set_edge_attributes(G, edge_labels, 'label')\n",
    "    \n",
    "    # Position nodes using spring layout\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Extract node and edge information\n",
    "    node_x = [pos[node][0] for node in G.nodes()]\n",
    "    node_y = [pos[node][1] for node in G.nodes()]\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_text = []\n",
    "    for edge in G.edges():\n",
    "        source, target = edge\n",
    "        x0, y0 = pos[source]\n",
    "        x1, y1 = pos[target]\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "        edge_text.append(G.get_edge_data(*edge)['label'])  # Use existing edge labels\n",
    "    \n",
    "    # Create a Plotly figure\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        mode='lines',\n",
    "        line_shape='spline',\n",
    "        opacity=0.5,\n",
    "        hoverinfo='none'\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        hovertext=[f'Node {node}' for node in G.nodes()]\n",
    "    ),\n",
    "    go.Scatter(\n",
    "        x=[(pos[edge[0]][0] + pos[edge[1]][0]) / 2 for edge in G.edges()],\n",
    "        y=[(pos[edge[0]][1] + pos[edge[1]][1]) / 2 for edge in G.edges()],\n",
    "        mode='text',\n",
    "        text=edge_text,\n",
    "        textposition='middle center',\n",
    "        hoverinfo='none'\n",
    "    )])\n",
    "    \n",
    "    # Customize the layout\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        hovermode='x',\n",
    "        margin=dict(b=20, l=5, r=5, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb589ef-f268-4be7-81fd-dabe969a12d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DDI Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb7bbc-35d7-4602-95ec-987c7ce468f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cas_number(cas_number):\n",
    "    url = f\"https://commonchemistry.cas.org/api/detail?cas_rn={cas_number}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()[\"name\"]\n",
    "        if data:\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def CID_information(CID: int):\n",
    "    cleaned_cid = int(re.sub(r\"CID[m|s]*0*\", \"\", CID))\n",
    "    compound = pcp.Compound.from_cid(cleaned_cid)\n",
    "    \n",
    "    try:\n",
    "        compound_name = cs.get_compound(cs.filter_results(cs.filter_inchikey(compound.inchikey))[0]).common_name\n",
    "    except:\n",
    "        if compound.synonyms:\n",
    "            flag = 0\n",
    "            for name in compound.synonyms:\n",
    "                if not re.search(r\"^[a-zA-Z\\s]+$\", name):\n",
    "                    flag = 1\n",
    "                    compound_name = name\n",
    "                    break\n",
    "\n",
    "            if flag == 0:\n",
    "                compound_name = compound.synonyms[0]\n",
    "        \n",
    "                # Checking if the returned name is a valid CAS number\n",
    "                if re.search(r\"\\b\\d{2,7}-\\d{2}-\\d\\b\", compound_name):\n",
    "                    cas_name = search_cas_number(compound_name)\n",
    "                    if (cas_name is not None) and (re.search(r\"^[a-zA-Z\\s]+$\", cas_name)):\n",
    "                        compound_name = cas_name\n",
    "        else:\n",
    "            compound_name = compound.iupac_name\n",
    "\n",
    "    return (compound_name, compound.canonical_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2932d4a-4fc7-40a4-b485-282ff9b91cec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run this cell only once! - Create and save the deterministic subgraph dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41a15e-93ba-4add-90c6-d568f3fd4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi = pd.read_csv(\"../data/base_data/bio-decagon-combo.csv\")\n",
    "\n",
    "G = nx.from_pandas_edgelist(ddi, 'STITCH 1', 'STITCH 2')\n",
    "\n",
    "# Get the largest connected component\n",
    "largest_component = max(nx.connected_components(G), key=len)\n",
    "\n",
    "largest_component.remove(\"CID006398525\") # This node has NO synonym, common name, chemspider entry, etc.\n",
    "\n",
    "# Sample nodes from the largest component\n",
    "sample_size = 500\n",
    "sample_nodes = np.random.choice(list(largest_component), size=sample_size, replace=False)\n",
    "\n",
    "# Get the subgraph induced by the sampled nodes\n",
    "subgraph = G.subgraph(sample_nodes)\n",
    "\n",
    "# Convert the subgraph back to a DataFrame\n",
    "sampled_df = nx.to_pandas_edgelist(subgraph)\n",
    "\n",
    "sampled_df.rename(columns={\"source\": \"STITCH 1\", \"target\": \"STITCH 2\"}, inplace=True)\n",
    "\n",
    "# Merging the sampled df with base ddi to get the side effect names\n",
    "sampled_df = pd.merge(sampled_df, ddi).drop_duplicates(subset=[\"STITCH 1\", \"STITCH 2\"])\n",
    "\n",
    "sampled_df.to_csv(\"../data/mined_data/sampled_ddi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc8eaf-e5ef-4c18-8fdf-d974ab2653c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check the output\n",
    "\n",
    "# Visualizing sampled_df as a graph to see that there are no disconnected edges\n",
    "create_graph_viz(sampled_df, \"STITCH 1\", \"STITCH 2\")\n",
    "\n",
    "all_drugs_according_to_df = len(set(sampled_df['STITCH 1'].to_list()).union(set(sampled_df['STITCH 2'].to_list())))\n",
    "\n",
    "# Verify that you get the same value from the sampled_df as a graph\n",
    "G = nx.from_pandas_edgelist(sampled_df, 'STITCH 1', 'STITCH 2')\n",
    "number_of_drug_nodes = len(G.nodes())\n",
    "\n",
    "assert all_drugs_according_to_df == number_of_drug_nodes\n",
    "\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4b83f-cf73-47c1-825a-e6c619409007",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Run these cells to get the drug info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d22710-a8f4-49ff-994f-e799b66b65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = pd.read_csv(\"../data/mined_data/sampled_ddi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109eb20-1e03-4d05-b0d6-56951c391fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drugs = set(sampled_df[\"STITCH 1\"].unique()).union(set(sampled_df[\"STITCH 2\"].unique()))\n",
    "drug_info = {}\n",
    "for drug_cid in tqdm(all_drugs):\n",
    "    drug_info[drug_cid] = CID_information(drug_cid)\n",
    "\n",
    "# Saving so that I don't have to requery API (rate limits)\n",
    "with Path(\"../data/mined_data/drug_annotations.pkl\").open(\"wb\") as f:\n",
    "    pickle.dump(drug_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f3b79-4515-4a1d-b841-a9fa14cbde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"../data/mined_data/drug_annotations.pkl\").open(\"rb\") as f:\n",
    "    drug_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa77aa-279b-455f-a17e-7328ee06e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_rows = []\n",
    "for row in tqdm(sampled_df.itertuples(index=False)):\n",
    "    drug_1_CID = row[0]\n",
    "    drug_2_CID = row[1]\n",
    "    relationship = row[3]\n",
    "\n",
    "    drug_1_info = drug_info[drug_1_CID]\n",
    "    drug_2_info = drug_info[drug_2_CID]\n",
    "    \n",
    "    drug_1_name = drug_1_info[0]\n",
    "    drug_1_SMILES = drug_1_info[1]\n",
    "\n",
    "    drug_2_name = drug_2_info[0]\n",
    "    drug_2_SMILES = drug_2_info[1]\n",
    "\n",
    "    enhanced_rows.append((drug_1_CID, drug_1_name, drug_1_SMILES, relationship, drug_2_CID, drug_2_name, drug_2_SMILES))\n",
    "\n",
    "pd.DataFrame(data=enhanced_rows, \n",
    "             columns=[\"drug_1_CID\", \"drug_1_name\", \"drug_1_SMILES\", \"relationship\",\n",
    "                      \"drug_2_CID\", \"drug_2_name\", \"drug_2_SMILES\"]).to_csv(\"../data/mined_data/DDI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecf766-2dc6-4c8a-9c41-96070eb78024",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DPI Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8250a4-1864-458f-97de-6ec4d642f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the DPIs and their annotations\n",
    "\n",
    "dpi = pd.read_csv(\"../data/base_data/bio-decagon-targets-all.csv\")\n",
    "ddi_subset = pd.read_csv(\"../data/mined_data/DDI_subset.csv\")\n",
    "\n",
    "all_drugs = np.unique(np.concatenate((ddi_subset[\"drug_1_CID\"].unique(), ddi_subset[\"drug_2_CID\"].unique())))\n",
    "\n",
    "# This will return less number of drugs since NOT ALL DRUGS HAVE DPIS!\n",
    "dpis_needed = dpi.query(\"STITCH in @all_drugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d3ce6-93ca-4574-84eb-5d56c90d479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Genes to proteins\n",
    "unique_genes = dpis_needed[\"Gene\"].unique()\n",
    "gene_protein = {}\n",
    "for gene in tqdm(unique_genes):  \n",
    "    # Retrieving protein data from the given gene ID\n",
    "    url = f\"https://string-db.org/api/json/get_string_ids?identifiers={gene}&species=9606\"\n",
    "    try:\n",
    "        response = requests.get(url).json()[0]\n",
    "        gene_protein[gene] = (response[\"stringId\"], response[\"preferredName\"], response[\"annotation\"])  \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "with Path(\"../data/mined_data/protein_annotations.pkl\").open(\"wb\") as f:\n",
    "    pickle.dump(gene_protein, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca143fe-74ba-4771-aee3-3bfb915b815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"../data/mined_data/protein_annotations.pkl\").open(\"rb\") as f:\n",
    "    gene_protein = pickle.load(f)\n",
    "\n",
    "gene_protein_df = pd.DataFrame.from_dict(gene_protein, orient=\"index\").reset_index()\n",
    "gene_protein_df.rename(columns={\"index\": \"Gene\", 0: \"stringID\", 1: \"protein_name\", 2:\"protein_desc\"}, inplace=True)\n",
    "annotated_dpis = pd.merge(dpis_needed, gene_protein_df)\n",
    "annotated_dpis.rename(columns={\"STITCH\": \"item_id_a\", \"stringID\": \"item_id_b\"}, inplace=True)\n",
    "\n",
    "actions_subset = pd.read_csv(\"../data/base_data/actions.csv\")\n",
    "merged = pd.merge(annotated_dpis, actions_subset)\n",
    "merged.rename(columns={\"item_id_a\": \"cid\", \"item_id_b\": \"stringId\"}, inplace=True)\n",
    "\n",
    "# Adding drug names to the DPI dataframe for completeness\n",
    "d1_names = ddi_subset[[\"drug_1_CID\", \"drug_1_name\"]].set_index(\"drug_1_CID\")[\"drug_1_name\"].to_dict()\n",
    "d2_names = ddi_subset[[\"drug_2_CID\", \"drug_2_name\"]].set_index(\"drug_2_CID\")[\"drug_2_name\"].to_dict()\n",
    "drug_cid_df = pd.DataFrame({**d1_names, **d2_names}.items(), columns=[\"cid\", \"drug_name\"])\n",
    "\n",
    "pd.merge(merged, drug_cid_df).drop_duplicates().to_csv(\"../data/mined_data/DPI_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f170317-88fc-4b18-82a7-e4b866987186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dedicated relationship table\n",
    "\n",
    "dpi_subset = pd.read_csv(\"../data/mined_data/DPI_subset.csv\")\n",
    "\n",
    "dpi = dpi_subset.drop(columns=[\"Gene\", \"stringId\", \"protein_desc\", \"score\", \"a_is_acting\", \"cid\"])\n",
    "\n",
    "final_rows = []\n",
    "for row in dpi.itertuples(index=False):\n",
    "    if row[1] != row[2]:\n",
    "        final_rows.append((row.drug_name, f\"{row.mode} and {row.action}\", row.protein_name))\n",
    "\n",
    "pd.DataFrame(final_rows, \n",
    "             columns=[\"drug_name\", \"relationship\", \"protein_name\"]).drop_duplicates().to_csv(\"../data/mined_data/DPI_subset_with_rel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9464f-4901-4e65-a0c1-8609f223b394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Collecting Background information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7a97b-48d3-4bec-937a-ca7827f57abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_subset = pd.read_csv(\"../data/mined_data/DDI_subset.csv\")\n",
    "dpi_subset = pd.read_csv(\"../data/mined_data/DPI_subset.csv\")\n",
    "\n",
    "all_drugs = set(ddi_subset[\"drug_1_name\"].unique()).union(set(ddi_subset[\"drug_2_name\"].unique()))\n",
    "\n",
    "'''\n",
    "Had to filter the drugs since wikipedia was erroneoulsy mapping some identifiers which are PubChem specifc such as 12080-13-9, etc.\n",
    "Better to thus stick to commercial named drugs with a lot of info.\n",
    "'''\n",
    "filtered_drugs = list(filter(lambda x: re.search(r\"^[a-zA-Z\\s]+$\", x), all_drugs))\n",
    "\n",
    "all_proteins = dpi_subset[[\"protein_name\", \"protein_desc\"]].set_index(\"protein_name\")[\"protein_desc\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203d8ef-0d54-495f-8ce6-a5decc4abf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for reference sections and Wikipedia search parameters\n",
    "REFERENCE_SECTIONS = [\"== References ==\", \"== See also ==\", \"== Further reading ==\", \"== External links ==\"]\n",
    "SEARCH_RESULTS = 1\n",
    "\n",
    "def clean_doc(doc):\n",
    "    \"\"\"Clean and extract relevant content from a Wikipedia page.\"\"\"\n",
    "    return re.split(\"|\".join(REFERENCE_SECTIONS), doc)[0].strip()\n",
    "\n",
    "def get_wikipedia_drug_content(entity):\n",
    "    try:\n",
    "        return wikipedia.summary(entity, auto_suggest=False, redirect=False).strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_wikipedia_protein_content(protein_name, protein_desc):\n",
    "    try:\n",
    "        search_with_name = wikipedia.summary(protein_name, auto_suggest=False, redirect=False).strip()\n",
    "    except:\n",
    "        search_with_name = None\n",
    "\n",
    "    try:\n",
    "        search_with_desc = wikipedia.summary(protein_desc.split(\";\")[0], auto_suggest=False, redirect=False).strip()\n",
    "    except:\n",
    "        search_with_desc = None\n",
    "    \n",
    "    if search_with_name == search_with_desc:\n",
    "        return search_with_desc\n",
    "    elif search_with_desc:\n",
    "        return search_with_desc\n",
    "    else:\n",
    "        return search_with_name\n",
    "\n",
    "def create_drug_info(drug: str):\n",
    "    '''\n",
    "    AD - Administration and Dosage\n",
    "    AE - Adverse Effects\n",
    "    PK - Pharmacokinetics\n",
    "    PD - Pharmacology\n",
    "    CO - Complications\n",
    "    TU - Therapeutic Use\n",
    "    DE - Drug Effects\n",
    "    '''\n",
    "    drug_query = f'''((\\\"{drug}\\\"[MeSH Terms] AND AD[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND AE[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND PK[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND PD[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND CO[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND TU[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND DE[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND TO[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND CT[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND DT[majr]) OR \\\n",
    "(\\\"{drug}\\\"[MeSH Terms] AND PO[majr])) OR ((\\\"{drug}\\\"[TIAB]) AND (humans[MH]) AND (hasabstract)'''\n",
    "\n",
    "    # Getting relevant papers for given drug\n",
    "    with Entrez.esearch(db='pubmed', term=drug_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "\n",
    "    if paper_list == []:\n",
    "        return None\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    filtered_record_list = list(filter(lambda x: \"MH\" in x and not any(re.search(r\"animals*\", term, re.IGNORECASE) for term in x), record_list))\n",
    "    \n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = \"\"\n",
    "    for record in filtered_record_list:\n",
    "        try:\n",
    "            metadata.append((drug, record[\"PMID\"], record[\"TI\"], record[\"AU\"], record[\"MH\"], \n",
    "                             f\"https://pubmed.ncbi.nlm.nih.gov/{record[\"PMID\"]}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if len(metadata) == 5:\n",
    "            break\n",
    "\n",
    "    return (all_abstracts_string, metadata)\n",
    "\n",
    "def create_protein_info(protein: str, protein_desc: str):\n",
    "    '''\n",
    "    CH - chemistry\n",
    "    ME - metabolism\n",
    "    PH - physiology\n",
    "    GE - genetics\n",
    "    AN - Analysis\n",
    "    BI - Biosynthesis\n",
    "    CS - Chemical Synthesis\n",
    "    DF - Deficiency\n",
    "    '''\n",
    "    protein_full_form = protein_desc.split(\";\")[0]\n",
    "\n",
    "    protein_query = f'''((\\\"{protein_full_form}\\\"[MeSH Terms] AND CH[majr]) OR \\\n",
    "(\\\"{protein_full_form}\\\"[MeSH Terms] AND ME[majr]) OR \\\n",
    "(\\\"{protein_full_form}\\\"[MeSH Terms] AND PH[majr]) OR \\\n",
    "(\\\"{protein_full_form}\\\"[MeSH Terms] AND GE[majr]) OR \\\n",
    "(\\\"{protein_full_form}\\\"[MeSH Terms] AND AN[majr]) OR \\\n",
    "(\\\"{protein_full_form}\\\"[MeSH Terms] AND BI[majr]) OR \\\n",
    "(\\\"{protein_full_form}\\\"[MeSH Terms] AND CS[majr]) OR \\\n",
    "(\\\"{protein_full_form}\\\"[MeSH Terms] AND DF[majr])) OR ((\\\"{protein_full_form}\\\"[TIAB]) AND (humans[MH]) AND (hasabstract)'''\n",
    "\n",
    "    # Getting relevant papers for given protein\n",
    "    with Entrez.esearch(db='pubmed', term=protein_query, retmax=20, sort=\"relevance\") as handle:\n",
    "        paper_list = Entrez.read(handle)[\"IdList\"]\n",
    "\n",
    "    if paper_list == []:\n",
    "        return None\n",
    "    \n",
    "    # Getting a parsable record for each paper\n",
    "    with Entrez.efetch(db='pubmed', rettype='medline', retmode=\"text\", id=paper_list) as handle:\n",
    "        records = Medline.parse(handle)\n",
    "        record_list = []\n",
    "        for rec in records:\n",
    "            record_list.append(rec)\n",
    "\n",
    "    filtered_record_list = list(filter(lambda x: \"MH\" in x and not any(re.search(r\"animals*\", term, re.IGNORECASE) for term in x), record_list))\n",
    "    \n",
    "    # Collecting relevant information\n",
    "    metadata = []\n",
    "    all_abstracts_string = protein_desc + \"\\n\" # Adding the annotation information from STRING\n",
    "    for record in filtered_record_list:\n",
    "        try:\n",
    "            metadata.append((protein, record[\"PMID\"], record[\"TI\"], record[\"AU\"], record[\"MH\"], \n",
    "                             f\"https://pubmed.ncbi.nlm.nih.gov/{record[\"PMID\"]}/\"))\n",
    "            all_abstracts_string = all_abstracts_string + record[\"AB\"] + \"\\n\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if len(metadata) == 5:\n",
    "            break\n",
    "    \n",
    "    return (all_abstracts_string, metadata)\n",
    "\n",
    "def write_content(entity, content, entity_type, source):\n",
    "    file_path = Path(f\"../data/background_information_data/{entity_type}_data/{source}/{entity}.txt\")\n",
    "    try:\n",
    "        with file_path.open(\"w\") as f:\n",
    "            f.write(content)\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file for {drug}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadd0b9-1ef3-4af2-8202-0389dc12ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for idx, drug in tqdm(enumerate(filtered_drugs)):\n",
    "    wiki_content = get_wikipedia_drug_content(drug)      \n",
    "    #pubmed_content = create_drug_info(drug)\n",
    "    if wiki_content is not None:\n",
    "        write_content(drug, wiki_content, \"drug\", \"Wiki\")\n",
    "    \"\"\"if pubmed_content is not None:\n",
    "        write_content(drug, pubmed_content[0], \"drug\", \"PubMed\")\n",
    "        metadata.extend(pubmed_content[1])\"\"\"\n",
    "    if (idx % 10 == 0) and (idx != 0):\n",
    "        time.sleep(1)\n",
    "pd.DataFrame(metadata, columns=[\"drug_name\", \"pubmed_id\", \"title\", \"authors\", \n",
    "                                \"mesh_terms\", \"paper_url\"]).to_csv(\"../data/background_information_data/drug_data/PubMed/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049272d-cf78-46e9-ac8a-bb66a116f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for idx, (protein_name, protein_desc) in tqdm(enumerate(all_proteins.items())):\n",
    "    wiki_content = get_wikipedia_protein_content(protein_name, protein_desc)\n",
    "    #pubmed_content = create_protein_info(protein_name, protein_desc) \n",
    "    if wiki_content is not None:\n",
    "        write_content(protein_name, wiki_content, \"protein\", \"Wiki\")\n",
    "    \"\"\"if pubmed_content is not None:\n",
    "        write_content(protein_name, pubmed_content[0], \"protein\", \"PubMed\")\n",
    "        metadata.extend(pubmed_content[1])\"\"\"\n",
    "    if (idx % 10 == 0) and (idx != 0):\n",
    "        time.sleep(1)\n",
    "pd.DataFrame(metadata, columns=[\"protein_name\", \"pubmed_id\", \"title\", \"authors\", \n",
    "                                \"mesh_terms\", \"paper_url\"]).to_csv(\"../data/background_information_data/protein_data/PubMed/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a7156-c623-440a-95d1-fe203aed1675",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Filtering entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e997e6-6028-4021-9377-8ec6d001e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ents(entity_type):\n",
    "    entity_folder = Path(f\"../data/background_information_data/{entity_type}_data/Wiki/\")\n",
    "    all_ents = set()\n",
    "    \n",
    "    for file in entity_folder.iterdir():\n",
    "        all_ents.add(file.stem)\n",
    "    \n",
    "    return all_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2202d0d-398a-44aa-a1ab-ad3247f1f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_subset = pd.read_csv(\"../data/mined_data/DDI_subset.csv\")\n",
    "dpi_subset = pd.read_csv(\"../data/mined_data/DPI_subset_with_rel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d743638-7739-440a-9662-3031eff0f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_with_text = collect_ents(\"drug\")\n",
    "proteins_with_text = collect_ents(\"protein\")\n",
    "\n",
    "drugs_with_text.remove(\"Sodium chloride\")\n",
    "drugs_with_text.remove(\"Potassium chloride\")\n",
    "\n",
    "# Retaining those drug pairs where each drug has text\n",
    "final_DDIs = ddi_subset.query(\"drug_1_name in @drugs_with_text and drug_2_name in @drugs_with_text\").sample(n=500, random_state=42)\n",
    "\n",
    "# For those drugs that have text & DPIs.\n",
    "final_DPIs = dpi_subset.query(\"drug_name in @drugs_with_text and protein_name in @proteins_with_text\")\n",
    "\n",
    "final_DDIs.to_csv(\"../data/mined_data/final_DDI.csv\", index=False)\n",
    "final_DPIs.to_csv(\"../data/mined_data/final_DPI.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1766726-4a6b-4eff-90e4-bc6151c9b305",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hop relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ecb41-f71e-4b86-b505-b266fa65ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_subset = pd.read_csv(\"../data/mined_data/final_DDI.csv\")\n",
    "dpi_subset = pd.read_csv(\"../data/mined_data/final_DPI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de1049-81ca-44e7-9985-bdb63c250c00",
   "metadata": {},
   "source": [
    "## Drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057860a-ff9d-4ed2-988f-f9289da37185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddi_two_hops():\n",
    "    sampled_two_hops = set()\n",
    "    while len(sampled_two_hops) < 100:\n",
    "        sampled_row = ddi_subset.sample(n=1).iloc[0] #Sample a row\n",
    "        start_drug = sampled_row.drug_1_name\n",
    "        middle_drug = sampled_row.drug_2_name\n",
    "        \n",
    "        middle_drug_neighbors = ddi_subset.query(\"drug_1_name == @middle_drug\")\n",
    "        if not middle_drug_neighbors.empty: # Check to see if the 2nd drug has any neighbors\n",
    "            end_drug = middle_drug_neighbors.sample(n=1).iloc[0].drug_2_name\n",
    "            sampled_two_hops.add((start_drug, middle_drug, end_drug))\n",
    "    return sampled_two_hops\n",
    "\n",
    "def create_ddi_three_hops():\n",
    "    sampled_three_hops = set()\n",
    "    while len(sampled_three_hops) < 100:\n",
    "        sampled_row = ddi_subset.sample(n=1).iloc[0] #Sample a row\n",
    "        start_drug = sampled_row.drug_1_name\n",
    "        second_drug = sampled_row.drug_2_name\n",
    "        second_drug_neighbors = ddi_subset.query(\"drug_1_name == @second_drug\")\n",
    "        if not second_drug_neighbors.empty:\n",
    "            third_drug = second_drug_neighbors.sample(n=1).iloc[0].drug_2_name\n",
    "            third_drug_neighbors = ddi_subset.query(\"drug_1_name == @third_drug\")\n",
    "            if not third_drug_neighbors.empty:\n",
    "                end_drug = third_drug_neighbors.sample(n=1).iloc[0].drug_2_name\n",
    "                sampled_three_hops.add((start_drug, second_drug, third_drug, end_drug))\n",
    "    \n",
    "    return sampled_three_hops\n",
    "\n",
    "all_ddi_two_hops = list(create_ddi_two_hops())\n",
    "all_ddi_three_hops = list(create_ddi_three_hops())\n",
    "\n",
    "with Path(\"../data/mined_data/DDI_two_hop_list.pkl\").open(\"wb\") as file:\n",
    "    pickle.dump(all_ddi_two_hops, file)\n",
    "\n",
    "with Path(\"../data/mined_data/DDI_three_hop_list.pkl\").open(\"wb\") as file:\n",
    "    pickle.dump(all_ddi_three_hops, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9533f90-02e9-4c78-987e-83c4830c2d1f",
   "metadata": {},
   "source": [
    "## Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c6e18-ce4d-41b4-a981-9cda5e84ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dpi_two_hops():\n",
    "    sampled_two_hops = set()\n",
    "    while len(sampled_two_hops) < 100:\n",
    "        sampled_row = dpi_subset.sample(n=1).iloc[0]\n",
    "        target_protein = sampled_row.protein_name\n",
    "        second_drug = sampled_row.drug_name\n",
    "        \n",
    "        second_drug_incoming_connections = ddi_subset.query(\"drug_2_name == @second_drug\")\n",
    "        if not second_drug_incoming_connections.empty:\n",
    "            first_drug = second_drug_incoming_connections.sample(n=1).iloc[0].drug_1_name\n",
    "            sampled_two_hops.add((first_drug, second_drug, target_protein))\n",
    "    return sampled_two_hops\n",
    "\n",
    "def create_dpi_three_hops():\n",
    "    sampled_three_hops = set()\n",
    "    while len(sampled_three_hops) < 100:\n",
    "        sampled_row = dpi_subset.sample(n=1).iloc[0]\n",
    "        target_protein = sampled_row.protein_name\n",
    "        third_drug = sampled_row.drug_name\n",
    "\n",
    "        third_drug_incoming_connections = ddi_subset.query(\"drug_2_name == @third_drug\")\n",
    "        if not third_drug_incoming_connections.empty:\n",
    "            second_drug = third_drug_incoming_connections.sample(n=1).iloc[0].drug_1_name\n",
    "            second_drug_incoming_connections = ddi_subset.query(\"drug_2_name == @second_drug\")\n",
    "            if not second_drug_incoming_connections.empty:\n",
    "                start_drug = second_drug_incoming_connections.sample(n=1).iloc[0].drug_1_name\n",
    "                sampled_three_hops.add((start_drug, second_drug, third_drug, target_protein))\n",
    "    return sampled_three_hops\n",
    "\n",
    "all_dpi_two_hops = list(create_dpi_two_hops())\n",
    "all_dpi_three_hops = list(create_dpi_three_hops())\n",
    "\n",
    "with Path(\"../data/mined_data/DPI_two_hop_list.pkl\").open(\"wb\") as file:\n",
    "    pickle.dump(all_dpi_two_hops, file)\n",
    "\n",
    "with Path(\"../data/mined_data/DPI_three_hop_list.pkl\").open(\"wb\") as file:\n",
    "    pickle.dump(all_dpi_three_hops, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
